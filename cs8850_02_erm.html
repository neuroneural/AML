<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link href="css/fontawesome-free-6.2.1-web/css/all.css" rel="stylesheet">

    <script src="lib/colorbrewer.v1.min.js" charset="utf-8"></script>
    <script src="lib/colorStringStandalone.js" charset="utf-8"></script>
    <script type="text/javascript" src="lib/jquery-2.2.4.min.js"></script>

    <title>Advanced Machine Learning</title>

    <meta name="description" content="CS8850 GSU class">
    <meta name="author" content="Sergey M Plis">

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">



    <link rel="stylesheet" href="dist/reset.css">
    <link rel="stylesheet" href="dist/reveal.css">
    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
    <!-- <link rel="stylesheet" href="lib/css/zenburn.css"> -->
    <link rel="stylesheet" href="css/custom.css">
    <link rel="stylesheet" href="dist/theme/aml.css" id="theme">
    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.scss';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
  </head>


  <body>
    <div class="reveal">
      <!-- In between the <div="reveal"> and the <div class="slides">-->
          <!-- <header style="position: absolute; top: 10px; left: 100px; z-index: 500; font-size:100px;background-color: rgba(0,0,0,0); text-align: center !important"></header>  -->
          <!-- In between the <div="reveal"> and the <div class="slides">-->
              <!-- Any section element inside of this container is displayed as a slide -->
              <div class="slides">

	        <section>
	          <section>
	            <p>
	              <h2>Advanced Machine Learning</h2>
                      <h3>02: foundations</h3>
	            <p>
	          </section>
	          <section>
	            <h3>Outline for the lecture</h3>
                    <ul>
                      <li class="fragment roll-in">Problem statement
                      <li class="fragment roll-in">Inductive bias
                      <li class="fragment roll-in">Bound for the probability of error
	            </ul>
                  </section>
                </section>
                <section>

                  <section>
                    <h2>Problem Statement</h2>
                  </section>
                  <section data-background-iframe="https://www.youtube.com/embed/E-lbpHIkaTo?autoplay=1&controls=0&rel=0&modestbranding=1&showinfo=0&mute=1&start=449" data-background-size="cover">
                    <img style="border:0; box-shadow: 0px 0px 0px
                                rgba(150, 150, 255, 0.8);" height="100%" class="contain"
                         src="figures/checking_papaya.jpg" alt="LA">

                    <aside class="notes">
                      Imagine  you have  just  arrived in  some small  Pacific
                      island. You soon find out that papayas are a significant
                      ingredient in  the local  diet. However, you  have never
                      before tasted papayas. You have  to learn how to predict
                      whether  a papaya  you see  in  the market  is tasty  or
                      not.  First, you  need  to decide  which  features of  a
                      papaya your prediction should be based on.  On the basis
                      of  your  previous  experience with  other  fruits,  you
                      decide to use two  features: the papaya’s color, ranging
                      from dark green,  through orange and red  to dark brown,
                      and  the papaya’s  softness, ranging  from rock  hard to
                      mushy. Your input for  figuring out your prediction rule
                      is a sample of papayas  that you have examined for color
                      and softness and then tasted  and found out whether they
                      were  tasty  or not.  Let  us  analyze  this task  as  a
                      demonstration of the considerations involved in learning
                      problems.
                      <br>
                      from  Shalev-Shwartz  S, Ben-David  S.   ``Understanding
                      machine learning: From theory to algorithms.'' Cambridge
                      university press; 2014
                    </aside>
                  </section>

                  <section>
                    <h3>setup</h3>
                    <ul  data-fragment-index="0">
                      <li class="fragment" data-fragment-index="1" >Domain Set ${\cal X}$ (all papayas)
                      <li class="fragment" data-fragment-index="2">Label set ${\cal Y}$
                      <li class="fragment" data-fragment-index="3">Training data
                        <ul>
                          $S = ((x_1,y_1)\dots (x_m,
                          y_m))$. <br>Sequence of pairs in
                          $\cal{X}\times\cal{Y}$
                        </ul>
                      <li class="fragment" data-fragment-index="4">The learner's output
                        <ul>
                          $h: \cal{X} \rightarrow \cal{Y}$
                        </ul>
                      <li class="fragment" data-fragment-index="5">A (simple) data-generation model
                        <ul>
                          <li class="fragment" data-fragment-index="6"> $\cal{D}$ - distribution of
                            papayas
                          <li class="fragment" data-fragment-index="7"> $f: \cal{X} \rightarrow
                            \cal{Y}$ - true labeling
                            function
                          <li class="fragment" data-fragment-index="8"> $y_i = f(x_i) \forall i$
                        </ul>

                    </ul>
                    <aside class="notes">
                      Domain set is all papayas in the example. <br>
                      Label set for papayas is $\{0, 1\}$, where 1 - tasty<br>
                      The sequence $S$ is often called a <i>training set</i><br>
                      Prediction rule, classifier, hypothesis, predictor<br>
                      Probability distribution that generates papayas is the environment. We assume some "correct" labeling function does exist but it is unknown to the learner. The learner's goal is to figure out this function.
                    </aside>
                  </section>
                  <section>
                    <h3>Measure of success: true loss</h3>
                    <div class="fragment" data-fragment-index="0" >
                      for $A \subset \cal{X}$, ${\cal D}(A)$ is
                      how likely to observe $x\in A$</div>
                    <div class="fragment" data-fragment-index="1" >
                      $A$ is an event expressed as
                      $\pi:{\cal X}\rightarrow \{0,1\}$</div>
                    <div class="fragment" data-fragment-index="2" >
                      $A = \{x\in {\cal X}: \pi(x) = 1\}$
                    </div>
                    <div class="fragment" data-fragment-index="3" >
                      then $\mathbb{P}_{x \sim {\cal D}}[\pi(x)]$ expresses ${\cal D}(A)$
                    </div>
                    <div class="fragment" data-fragment-index="4" >
                      $L_{({\cal D},f)}(h)
                      \stackrel{\text{def}}{=}\mathbb{P}_{x
                      \sim {\cal D}}[h(x) \ne f(x)]$
                    </div>
                    <div class="fragment" data-fragment-index="5" >
                      generalization error, the risk, the
                      true error of $h$, or loss!
                    </div>
                    <aside class="notes">
                      Loss is the assumed true loss here on the whole domain.
                    </aside>
                  </section>
                  <section>
                    <h3>Measure of success: empirical loss</h3>
                    <ul>
                      <li>Training set $S\sim {\cal D}$
                      <li>Expected predictor $h_s: \cal{X} \rightarrow
                        \cal{Y}$
                      <li>Find $h_S$ that minimizes a loss with
                        respect to unknown ${\cal D}$ and $f$
                    </ul>
                    <div class="fragment"
                         data-fragment-index="0" >
                      $L_{S}(h) \stackrel{\text{def}}{=}
                      \frac{\mid\{i\in\{1,\dots,m\}:h(x_i) \ne y_i\}\mid}{m}$
                    </div>
                    <div class="fragment"
                         data-fragment-index="0" >
                      Empirical Risk Minimization
                    </div>
                    <aside class="notes">
                      Given a training set $S$, which is a randomly samples subset of ${\cal X}$, what can we optimize?<br>
                      We can optimize empirical loss: error with respect to the samples $(x_i,y_i) \in S \forall i$.<br>
                      We may be able to estimate a classifier that does well on the training set, but what we really want is a classifier that does well with respect to the true loss.
                    </aside>
                  </section>
                  <section>
                    <h3>What can go wrong?</h3>
                  </section>
                  <section>
                    <h3 class="fragment"
                        data-fragment-index="2" >overfitting</h3>
                    <img style="border:0; box-shadow: 0px 0px 0px
                                rgba(150, 150, 255, 0.8);" width="350"
                         src="figures/overfitting2.png" alt="perf">
                    <div class="fragment"
                         data-fragment-index="0" >
                      $h_S(x) = \begin{cases}
                      y_i & \text{if } \exists i \in \{1,\dots,m\} s.t. x_i=x\\
                      0 & \text{otherwise}
                      \end{cases}
                      $
                    </div>
                    <div class="fragment"
                         data-fragment-index="1" >
                      $L_{S}(h_S) = 0$ yet $L_{\cal D}(h_S) = 1/2$
                    </div>
                    <aside class="notes">
                      For simplicity imagine that our task is as such: an area 1 square is inscribed into a 2D square of area 2, elements of one class are present only inside the inscribed square, while elements of the other class are always outside.
                    </aside>
                  </section>
                </section>
                  <section>
                    <section>
                      <h2>Inductive bias</h2>
                    </section>
                    <section data-vertical-align-top data-background-iframe="https://www.youtube.com/embed/JOMrP6sHXPg?autoplay=1&controls=0&rel=0&modestbranding=1&showinfo=0&mute=1" data-background-size="cover">
                      <h3 style="text-shadow: 4px 4px 4px #002b36; color: #93a1a1">Bait shyness</h3>
		      <aside class="notes">
			rats try a bit of new food and if they get sick - never again it that food
			<br>
			pidgeons practice inductive reasoning, proof by induction,
			<br>

		      </aside>
                    </section>

                    <section data-vertical-align-top data-background-iframe="https://www.youtube.com/embed/NCtF4aVlxgU?autoplay=1&controls=0&rel=0&modestbranding=1&showinfo=0" data-background-size="cover">
                      <h3 style="text-shadow: 4px 4px 4px #002b36; color: #93a1a1">Pigeon superstition</h3>
                    </section>

                    <section data-vertical-align-top data-background-iframe="https://www.youtube.com/embed/xt-ycTMISwg?autoplay=1&controls=0&rel=0&modestbranding=1&showinfo=0" data-background-size="cover">
                      <h3 style="text-shadow: 4px 4px 4px #002b36; color: #93a1a1">B. F. Skinner conditions a pigeon</h3>
		      <aside class="notes">

		      </aside>
                    </section>

                    <section data-vertical-align-top>
                      <h3>What about rat superstition and conditioning?</h3>
                      <div class="fragment" data-fragment-index="1">
                      <img width="350" src="figures/JerryLaughing.gif" alt="Jerry">
                      </div>
		      <aside class="notes">
			can we electroshock rats and make them to dislike the food?<br>
			only if nauseated they stop eating - the rest does not matter<br>
			- pidgeon overfits <br>
			- rat - has an inductive bias
		      </aside>
                    </section>

                    <section>
                      <h3>What can we do in our case?</h3>
                      <ul  data-fragment-index="0">
                        <li class="fragment" data-fragment-index="1" >Search over a restricted search space.
                          <ul>
                            <li> Choose <i>hypothesis  class</i>  ${\cal H}$ in advance
                            <li> $\forall h\in {\cal H}, h: \cal{X} \rightarrow \cal{Y}$
                          </ul>
                      </ul>
                    </section>
                    <section>
                      <h3>Inductive bias</h3>
                      <dev>
                        $\text{ERM}_{\cal H}(S) \stackrel{\text{def}}{=} \underset{h\in{\cal H}}{\argmin}L_S(h)$
                      </dev><br>
                      <dev>
                        <b>bias</b> the learner to a particular set of predictors
                      </dev>
                      <aside class="notes">
                        Let's limit the learner to a hypothesis class. This may make sense if we think about picking a model class, such as linear classifiers.
                      </aside>
                    </section>

                    <section>
                      <h3>Other inductive biasi</h3>
                      <ul  data-fragment-index="0">
                        <li class="fragment" data-fragment-index="1" >Maximum conditional independence<br>
                          <i>cast in a Bayesian framework, try to maximize conditional independence (Naive Bayes)</i>
                        <li class="fragment" data-fragment-index="2" >Minimum cross-validation error
                        <li class="fragment" data-fragment-index="3" >Maximum margin
                        <li class="fragment" data-fragment-index="4" >Minimum description length
                        <li class="fragment" data-fragment-index="5" >Minimum features
                        <li class="fragment" data-fragment-index="6" >Nearest neighbors
                      </ul>
                      <aside class="notes">
                        We will learn or refresh some of these during the course, but most you may already know from ML intro courses. Remember, this is an advance course and although I do not require much prior knowledge from you, I often assume it.
                      </aside>
                    </section>
                    <section>
                      <h3>Finite Hypothesis Classes</h3>
                      <dev>
                        $h_S \in \underset{h\in{\cal H}}{\argmin}L_S(h)$
                      </dev><br>
                    </section>
                  </section>

                  <section>
                    <section>
                      <h2>Bound the probability of error</h2>
                    </section>
                  <section>
                    <h3>Assumptions</h3>
                    <!-- <dev> -->
                    <!--   $h_S \in \underset{h\in{\cal H}}{\argmin}L_S(h)$ -->
                    <!-- </dev><br> -->
                    <blockquote style="text-align: left;" style="background-color: #eee8d5;" class="fragment" data-fragment-index="0">
                      <b>The Realizability Assumption:</b> There exists $h^* \in {\cal H} s.t. L_{{\cal D}, f}(h^*)=0$. This implies: with probability 1 over random samples $S\sim {\cal D}$ labeled by $f$, we have $L_S(h^*)=0$
                    </blockquote>
                    <blockquote style="text-align: left;" style="background-color: #eee8d5;" class="fragment" data-fragment-index="1">
                      <b>The i.i.d. Assumption:</b> Samples in the training set are independent and identically distributed. Denoted as $S\sim {\cal D}^m$
                    </blockquote>
                    <aside class="notes">
                      ${\cal D}^m$ denotes the probability over $m$-tuples induces by  applying ${\cal D}$ to pick each element of the tuple independently of the other members of the tuple.
                    </aside>
                  </section>

                  <section>
                    <h3>Confidence and accuracy</h3>
                    <div class="fragment" data-fragment-index="0" >
                      The risk $L_{({\cal D},f)}(h_S)$ depends on the randomly picked training set $S$. We say, the risk is a random variable.
                    </div>
                    <div class="fragment" data-fragment-index="1" >
                      Some training sets $S$ can be really bad! Denote the probability of getting a <b>nonrepresentative sample</b> by $\delta$
                    </div>
                    <div class="fragment" data-fragment-index="2" >
                      Let's call $(1-\delta)$ - <i>confidence parameter</i>
                    </div>
                    <div class="fragment" data-fragment-index="3" >
                      Can't hope to always have perfect loss $L_{({\cal D}, f)}=0$. Let's introduce the <i>accuracy</i> parameter $\epsilon$.
                    </div>
                    <div class="fragment" data-fragment-index="4" >
                      <blockquote>
                        Failure is when $L_{({\cal D}, f)}>\epsilon$ <br> Success is when $L_{({\cal D}, f)}\le \epsilon$
                      </blockquote>
                    </div>
                  </section>

                  <section>
                    <h4>What is the probability of a bad sample that fails the learner?</h4>
                    <div class="fragment" data-fragment-index="0" >
                      $S\mid_x = (x_1, x_2, \dots, x_m)$ - instances of the training set
                    </div>
                    <br>
                    <div class="fragment" data-fragment-index="1" >
                      We want to upperbound ${\cal D}^m(\{S\mid_x : L_{({\cal D}, f)}(h_S) > \epsilon\})$
                    </div>
                    <br>
                    <div class="fragment" data-fragment-index="2" >
                      The set of "bad" hypotheses ${\cal H}_B = \{h\in {\cal H} : L_{({\cal D}, f)}(h) > \epsilon\})$
                    </div>
                    <br>
                    <div class="fragment" data-fragment-index="3" >
                      Set of misleading samples $M = \{S\mid_x : \exists h \in {\cal H}_B, L_S(h)=0\}$
                    </div>
                  </section>
                  <section>
                    <h3>But remember our assumption?</h3>
                    <blockquote style="text-align: left;">
                      <b>The Realizability Assumption:</b> There exists $h^* \in {\cal H} s.t. L_{{\cal D}, f}(h^*)=0$. This implies: with probability 1 over random samples $S\sim {\cal D}$ labeled by $f$, we have $L_S(h^*)=0$
                    </blockquote>
                    Means $L_S(h_S) = 0$, where $h_S \in \underset{h\in{\cal H}}{\argmin}L_S(h)$. Hence $L_{({\cal D}, f)}(h_S) > \epsilon$ can only happen if for some $h\in {\cal H}_B$, $L_S(h)=0$
                    <br>
                    <div class="fragment" data-fragment-index="0" >
                      follows $\{S\mid_x : L_{({\cal D}, f)}(h_S) > \epsilon \} \subseteq M$
                    </div>
                  </section>
                  <section>
                    <div class="fragment" data-fragment-index="0" >
                      We want to upperbound ${\cal D}^m(\{S\mid_x : L_{({\cal D}, f)}(h_S) > \epsilon\})$
                    </div>
                    <div class="fragment" data-fragment-index="1" >
                      $\{S\mid_x : L_{({\cal D}, f)}(h_S) > \epsilon \} \subseteq M$
                    </div>
                    <div class="fragment" data-fragment-index="2" >
                      rewrite set of misleading samples
                      $M = \{S\mid_x : \exists h \in {\cal H}_B, L_S(h)=0\}$
                    </div>
                    <div class="fragment" data-fragment-index="3" >
                      as $M = \underset{h\in {\cal H}_B}\bigcup \{S\mid_x : L_{S}(h) = 0  \}$
                    </div>
                    <div class="fragment" data-fragment-index="4" >
                      ${\cal D}^m(\{S\mid_x : L_{({\cal D}, f)}(h_S) > \epsilon\}) \le {\cal D}^m(M)$
                    </div>
                    <div class="fragment" data-fragment-index="5" >
                      ${\cal D}^m(M) = {\cal D}^m(\underset{h\in {\cal H}_B}\bigcup \{S\mid_x : L_{S}(h) = 0  \})$
                    </div>
                  </section>
                  <section>
                    <h3>Confidence and accuracy</h3>
                    <blockquote style="text-align: left; width: 100%;">
                      <b>Union Bound:</b> For any two sets $A$, $B$ and a distribution $\cal D$ we have
                      ${\cal D}(A\cup B) \le {\cal D}(A) + {\cal D}(B)$
                    </blockquote>
                    <div class="fragment" data-fragment-index="0">
                    hence<br>
                    ${\cal D}^m(\{S\mid_x : L_{({\cal D}, f)}(h_S) > \epsilon\}) \le {\cal D}^m(\underset{h\in {\cal H}_B}\bigcup \{S\mid_x : L_{S}(h) = 0  \})$
                    <br>$\le \sum_{h\in {\cal H}_B}{\cal D}^m(\{S\mid_x : L_{S}(h) = 0  \})$
                    </div>
                    <div class="fragment" data-fragment-index="1">
                      let's put a bound on each summand separately
                    </div>
                  </section>

                  <section>
                    <div id="header-right" style="margin-right:  -250px; margin-top: -20px" class="fragment" data-fragment-index="3">
                      <img src="figures/inequality_eps.png" alt="inequality" style="border:0; box-shadow: 0px 0px 0px rgba(150, 150, 255, 1); margin-bottom: -5%" width="300px" >
                    </div>
                    <h3>Confidence and accuracy</h3>
                    <div class="fragment" data-fragment-index="0" >
                      Let's put a bound on each summand separately<br>
                      ${\cal D}^m(\{S\mid_x : L_{S}(h) = 0  \}) = $<br>
                      ${\cal D}^m(\{S\mid_x : \forall i, h(x_i) = f(x_i)  \})$
                      <br>$= \prod_{i=1}^m {\cal D}(\{x_i: h(x_i) = f(x_i)\})$
                    </div>

                    <div class="fragment" data-fragment-index="1" >
                      remember we are only considering bad hypotheses<br>
                      $h\in{\cal H}_B = \{h\in {\cal H} : L_{({\cal D}, f)}(h) > \epsilon\}$
                    </div>

                    <div class="fragment" data-fragment-index="2" >
                      ${\cal D}(\{x_i: h(x_i) = f(x_i)\}) = 1 - L_{({\cal D}, f)}(h) \le 1-\epsilon$
                    </div>

                    <div class="fragment" data-fragment-index="3" >
                      using $1-\epsilon \le e^{-\epsilon}$<br>
                      ${\cal D}^m(\{S\mid_x : L_{S}(h) = 0  \}) \le (1-\epsilon)^m \le e^{-\epsilon m}$
                    </div>

                  </section>

                  <section>
                    <h3>the final bound</h3>
                    <ul  style="list-style-type: none; font-size: 0.9em">
                      <li class="fragment fade-in-then-semi-out"> ${\cal D}^m(\{S\mid_x : L_{S}(h) = 0  \}) \le e^{-\epsilon m}$
                      <li class="fragment fade-in-then-semi-out">
                        ${\cal D}^m(\{S\mid_x : L_{({\cal D}, f)}(h_S) > \epsilon\}) \le \sum_{h\in {\cal H}_B}{\cal D}^m(\{S\mid_x : L_{S}(h) = 0  \})$
                      <li class="fragment fade-in-then-semi-out">
                        $\sum_{h\in {\cal H}_B}{\cal D}^m(\{S\mid_x : L_{S}(h) = 0  \}) \le \mbox{ ?}$
                      <li class="fragment fade-in-then-semi-out">
                        $\sum_{h\in {\cal H}_B}{\cal D}^m(\{S\mid_x : L_{S}(h) = 0  \}) \le |{\cal H}_B|e^{-\epsilon m}$
                      <li class="fragment fade-in-then-semi-out">
                        $|{\cal H}_B|e^{-\epsilon m}\le |{\cal H}|e^{-\epsilon m}$
                      <li class="fragment fade-in-then-semi-out">
                        ${\cal D}^m(\{S\mid_x : L_{({\cal D}, f)}(h_S) > \epsilon\}) \le \sum_{h\in {\cal H}_B}{\cal D}^m(\{S\mid_x : L_{S}(h) = 0  \})$
                      <li class="fragment fade-in-then-semi-out">
                        <blockquote shade style="margin-top: 20px; text-align: center; width: 100%;">
                          ${\cal D}^m(\{S\mid_x : L_{({\cal D}, f)}(h_S) > \epsilon\}) \le |{\cal H}|e^{-\epsilon m}$
                        </blockquote>
                    </ul>
                  </section>
                </section>

              </div>

            </div>

            <script src="dist/reveal.js"></script>

            <link rel="stylesheet" href="lib/css/monokai.css">
            <script src="plugin/highlight/highlight.js"></script>
            <script src="plugin/math/math.js"></script>
            <script src="plugin/chalkboard/plugin.js"></script>
            <script src="plugin/notes/notes.js"></script>
            <script src="plugin/zoom/zoom.js"></script>

            <script>
              // Full list of configuration options available at:
              // https://github.com/hakimel/reveal.js#configuration

              Reveal.initialize({
                  //history: true,
                  hash: true,
                  margin: 0.01,
                  minScale: 0.01,
                  maxScale: 1.,

                  chalkboard: {
                      boardmarkerWidth: 1,
                      chalkWidth: 2,
                      chalkEffect: 1,
                      slideWidth: Reveal.width,
                      slideHeight: Reveal.height,
                      // src: "chalkboards/chalkboard_em2.json",
                      readOnly: false,
                      theme: "blackboard",
                      eraser: { src: "plugin/chalkboard/img/sponge.png", radius: 30},
                  },

                  math: {
                      mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
                      config: 'TeX-AMS_SVG-full',
                      // pass other options into `MathJax.Hub.Config()`
                      TeX: {
                          Macros: {
        	              RR: '\\mathbb{R}',
        	              PP: '\\mathbb{P}',
        	              EE: '\\mathbb{E}',
        	              NN: '\\mathbb{N}',
        	              vth: '\\vec{\\theta}',
                              loss: '{\\cal l}',
                              hclass: '{\\cal H}',
                              CD: '{\\cal D}',
                              def: '\\stackrel{\\text{def}}{=}',
                              pag: ['\\text{pa}_{{\cal G}^{#1}}(#2)}', 2],
                              vec: ['\\boldsymbol{\\mathbf #1}', 1],
        	              set: [ '\\left\\{#1 \\; : \\; #2\\right\\}', 2 ],
                              bm: ['\\boldsymbol{\\mathbf #1}', 1],
                              argmin: ['\\operatorname\{arg\\,min\\,\}'],
                              argmax: ['\\operatorname\{arg\\,max\\,\}'],
                              prob: ["\\mbox{#1$\\left(#2\\right)$}", 2],
                          },
                          loader: {load: ['[tex]/color']},
                          extensions: ["color.js"],
                          tex: {packages: {'[+]': ['color']}},
                          svg: {
                              fontCache: 'global'
                          }
                      }
                  },

                  plugins: [ RevealMath, RevealChalkboard, RevealHighlight, RevealNotes, RevealZoom ],

              });

              Reveal.configure({ fragments: true }); // set false when developing to see everything at once
              Reveal.configure({ slideNumber: true });
              //Reveal.configure({ history: true });
              Reveal.configure({ slideNumber: 'c / t' });
              Reveal.addEventListener( 'darkside', function() {
                  document.getElementById('theme').setAttribute('href','dist/theme/aml_dark.css');
              }, false );
              Reveal.addEventListener( 'brightside', function() {
                  document.getElementById('theme').setAttribute('href','dist/theme/aml.css');
              }, false );

            </script>

            <style type="text/css">
              /* 1. Style header/footer <div> so they are positioned as desired. */
              #header-left {
                  position: absolute;
                  top: 0%;
                  left: 0%;
              }
              #header-right {
                  position: absolute;
                  top: 0%;
                  right: 0%;
              }
              #footer-left {
                  position: absolute;
                  bottom: 0%;
                  left: 0%;
              }
            </style>

            <!-- // 2. Create hidden header/footer -->
            <div id="hidden" style="display:none;">
              <div id="header">
                <div id="header-left"><h4>CS8850</h4></div>
                <div id="header-right"><h4>Advanced Machine Learning</h4></div>
                <div id="footer-left">
                  <img style="border:0; box-shadow: 0px 0px 0px rgba(150, 150, 255, 1);" width="200"
                       src="figures/valentino.png" alt="robot learning">
                </div>
              </div>
            </div>


            <script type="text/javascript">
              // 3. On Reveal.js ready event, copy header/footer <div> into each `.slide-background` <div>
              var header = $('#header').html();
              if ( window.location.search.match( /print-pdf/gi ) ) {
                  Reveal.addEventListener( 'ready', function( event ) {
                      $('.slide-background').append(header);
                  });
              }
              else {
                  $('div.reveal').append(header);
              }
            </script>

  </body>
</html>
